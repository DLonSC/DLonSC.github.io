The Deep Learning for Science workshop is with ISC'19 on June 20th, 2019 in Frankfurt, Germany. It is the second workshop in the Deep Learning on Supercomputers series. The workshop provides a forum for practitioners working on any and all aspects of DL for scientific research in the High Performance Computing (HPC) context to present their latest research results and development, deployment, and application experiences. The general theme of this workshop series is the intersection of DL and HPC, while the theme of this particular workshop is centered around the applications of deep learning methods in scientific research: novel uses of deep learning methods, e.g., convolutional neural networks (CNN), recurrent neural networks (RNN), generative adversarial network (GAN), and reinforcement learning (RL), for both natural and social science research, and innovative applications of deep learning in traditional numerical simulation. Its scope encompasses application development in scientific scenarios using HPC platforms; DL methods applied to numerical simulation; fundamental algorithms, enhanced procedures, and software development methods to enable scalable training and inference; hardware changes with impact on future supercomputer design; and machine deployment, performance evaluation, and reproducibility practices for DL applications with an emphasis on scientific usage.

Topics include but are not limited to:
- Emerging scientific applications driven by DL methods
- Novel interactions between DL and traditional numerical simulation
- Effectiveness and limitations of DL methods in scientific research
- Algorithms and procedures to enhance reproducibility of scientific DL applications
- Data management through the life cycle of scientific DL applications
- General algorithms and procedures for efficient and scalable DL training
- General algorithms and systems for large scale model serving for scientific use cases
- New software, and enhancements to existing software, for scalable DL
- DL communication optimization at scale
- I/O optimization for DL at scale
- Hardware (processors, accelerators, memory hierarchy, interconnect) changes with impact on deep learning in the HPC context
- DL performance evaluation and analysis on deployed systems
- DL performance modeling and tuning of DL on supercomputers
- DL benchmarks on supercomputers

As part of the reproducibility initiative, the workshop requires authors to provide information such as the algorithms, software releases, datasets, and hardware configurations used. For performance evaluation studies, we will encourage authors to use well-known benchmarks or applications with open accessible datasets: for example, [MLPerf](https://github.com/mlperf/training) and ResNet-50 with the [ImageNet-1K dataset](http://www.image-net.org/archive/stanford/fall11_whole.tar).

<!--- You can use the [editor on GitHub](https://github.com/DLonSC/DLonSC.github.io/edit/master/README.md) to maintain and preview the content for your website in Markdown files. -->

<!--- Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files. -->

### Import Dates

- Extended abstract due: ~~April 27th, 2019~~ May 4th, 2019
- Acceptance notification: May 11th, 2019
- Camera ready: June 1st, 2019
- Workshop date: June 20th, 2019

### Program

| Time | Title | Speaker |
| --- | --- | --- |
| 9:00--9:10 | Opening | Valeriu Codreanu, SURFSara |
| 9:10--9:40 | Deep learning workflows using CANDLE | Tom Brettin, Argonne National Laboratory |
| 9:40--10:10 | What is Unique in Individual Gait Patterns? Understanding and Interpreting - Deep<br/> Learning in Gait Analysis | Fabian Horst, University of Mainz |
| 10:10--10:40 | Collider event generation with deep generative models | Sydney Otten, Radboud University Nijmegen |
| 10:40--11:10 | Deep Learning/AI Accelerated Advances in<br/> Fusion Energy Science for Disruption<br/> Predictions with Implications for Plasma Control | Bill Tang, Princeton University |
| 11:00--11:30 | Coffee Break | |
| 11:30--12:00 | Generative Modeling of Protein Folding Transitions with Recurrent Auto-encoders | Arvind Ramanathan, Argonne National Laboratory |
| 12:00--12:30 | Neural Networks Predict Fluid Dynamics Solutions from Constrained Datasets | Cristina White, Stanford University |
| 12:30--13:00 | Accelerating the simulations of nonlinear dynamical systems in astrophysics with deep learning | Maxwell Cai, Leiden Observatory |
| 13:00--14:00 | Lunch Break | |
| 14:00--15:00 | Keynote, Deep Learning application for High Energy Physics: examples from the LHC | Sofia Vallecorsa, CERN |
| 15:00--15:30 | Understanding the Earth system with machine learning | Markus Reichstein, Max Planck Institute for Biogeochemistry|
| 15:30--16:00 | Machine-learned turbulence in next-generation weather models | Chiel van Heerwaarden, Wageningen University |
| 16:00--16:30 | Coffee Break | |
| 16:30--17:30 | Panel Discussion | |


### Paper Submission

Authors are invited to submit an extended abstract with 1-4 pages in single column text with LNCS style. All submissions should be in [LNCS format](http://www.springer.com/de/it-informatik/lncs/conference-proceedings-guidelines) and submitted using [easychair](https://easychair.org/conferences/?conf=dls2019).


### Organizing Committee
- Valeriu Codreanu (co-chair), SURFsara, Netherlands
- Ian Foster (co-chair), UChicago & ANL, USA
- Zhao Zhang (co-chair), TACC, USA
- Weijia Xu (proceeding chair), TACC, USA
- Takuya Akiba, Preferred Networks, Japan
- Thomas S. Brettin, ANL, USA
- Erich Elsen, DeepMind, USA
- Steve Farrell, LBNL, USA
- Song Feng, IBM Research, USA
- Boris Ginsburg, Nvidia, USA
- Torsten Hoefler, ETH, Switzerland
- Jessy Li, UT Austin, USA
- Zhengchun Liu, ANL, USA
- Peter Messmer, Nvidia, USA
- Damian Podareanu, SURFsara, Netherlands
- Simon Portegies Zwart, Leiden Observatory, Netherlands 
- Judy Qiu, Indiana University, USA
- Arvind Ramanathan, ORNL, USA
- Vikram Saletore, Intel, USA
- Mikhail E. Smorkalov, Intel, Russia
- Rob Schreiber, Cerebras, USA
- Dan Stanzione, TACC, USA
- Rick Stevens, UChicago & ANL, USA
- Wei Tan, Citadel, USA
- Jordi Torres, Barcelona Supercomputing Center, Spain
- Daniela Ushizima, LBNL, USA
- Sofia Vallecorsa , CERN, Switzerland
- David Walling, TACC, USA
- Markus Weimer, Microsoft, USA
- Kathy Yelick, UC Berkeley & LBNL, USA

### Previous Workshop
[1st Deep Learning on Supercomputers Workshop in SC'18 at Dallas, USA](https://www.tacc.utexas.edu/workshop/2018/deep-learning)
